{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import wikipedia\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# gensim\n",
    "from gensim.utils import simple_preprocess, dict_from_corpus\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import preprocess_string, preprocess_documents\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki-Pedia API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL',\n",
       " 'Apple Inc.',\n",
       " 'History of Apple Inc.',\n",
       " 'American Association of Professional Landmen',\n",
       " 'Option symbol',\n",
       " 'Energy management (degree)',\n",
       " 'Alpha Indexes',\n",
       " 'I Am Rich',\n",
       " 'Landman (oil worker)',\n",
       " 'Jeff Williams (Apple)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia.search(query='AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, and the HomePod smart speaker. Apple's software includes the macOS and iOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode. Its online services include the iTunes Store, the iOS App Store and Mac App Store, Apple Music, and iCloud.\n",
      "Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell Wozniak's Apple I personal computer. It was incorporated as Apple Computer, Inc. in January 1977, and sal\n"
     ]
    }
   ],
   "source": [
    "wikipage = wikipedia.page('Apple Inc.')\n",
    "print(wikipage.content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA -- Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note:\n",
    "1. In gensim implementation: you never pass raw text to gensim.models, (a piece of doc/sentence = list of strings), you have to tokenize and vectorize your doc from one giant string to list of tuples (word id, counts) first, this list of tuples is called \"bag of words\" representation, so then you can pass \"corpus\" (list of docs / list of list of tuples) to gensim.models\n",
    "2. How to transform a doc to bag of words (one element of corpus): first, tokenize the texts, now you have list of single-word / n-gram strings, second, use gensim.corpora.Dictionary to create a mapping between id and tokens(words/n-gram), lastly, use this dictionary that you just built to do doc2bow(['This', 'is', 'a', 'tokenized', 'document']) to convert tokenized doc to \"bag of words\" representation (list of tuples), and extend the list for all the docs, now you form a \"corpus\" (list of docs / list of list of tuples)\n",
    "3. How to transform docs to Tfidf corpus: first, transform docs to bag of words corpus. Second, instantiate TfidfModel(corpus=corpus, id2word=id2word). Last, transform bag of words corpus to Tfidf corpus via tfidf[corpus]\n",
    "4. How to tokenize n-grams in gensim: There is no n-gram implementation in gensim but there is collocation phrase detection implementation: use gensim.models.phrases.Phrases to train on list of docs (list of list of string tokens), use gensim.models.phrases.Phraser to tokenizer list of string tokens; Repeating above process again will yield tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare sentences for this doc\n",
    "sentences = nltk.sent_tokenize(wikipage.content)\n",
    "## tokenize sentences\n",
    "# tokens = [simple_preprocess(sent) for sent in sentences]\n",
    "tokens = [preprocess_string(sent) for sent in sentences]\n",
    "# ## tokenize sentences using collocation (bi-gram)\n",
    "# # first-run: train the bi-gram collocation detector\n",
    "# phrases = Phrases(sentences=tokens, min_count=5, threshold=10.0)\n",
    "# # create a performant Phraser object the execute the phrase model\n",
    "# bigram = Phraser(phrases)\n",
    "# tokens = [bigram[sent] for sent in tokens]\n",
    "# # second-run: train the tri-gram collocation detector (usually worse)\n",
    "# phrases = Phrases(sentences=tokens, min_count=1, threshold=1.0)\n",
    "# # create a performant Phraser object the execute the phrase model\n",
    "# bigram = Phraser(phrases)\n",
    "# tokens = [bigram[sent] for sent in tokens]\n",
    "## build id to word dictionary from tokens\n",
    "id2word = Dictionary(tokens)\n",
    "## build bag of word corpus\n",
    "corpus = [id2word.doc2bow(doc) for doc in tokens]\n",
    "# ## (optional) build Tfidf transformed corpus, which will hurt LDA performance)\n",
    "# tfidf = TfidfModel(corpus=corpus, id2word=id2word)\n",
    "# tfidf_corpus = tfidf[corpus]\n",
    "# corpus = tfidf_corpus\n",
    "## train model\n",
    "lda = LdaModel(corpus=corpus, num_topics=5, id2word=id2word, random_state=1)\n",
    "# lda.update(corpus=corpus)\n",
    "## print document topics and probas\n",
    "topics = [lda.get_document_topics(corpus[i], minimum_probability=0) for i in range(len(corpus))]\n",
    "## print the topics with max probas\n",
    "topic = [\n",
    "    max(\n",
    "        lda.get_document_topics(corpus[i], minimum_probability=0), \n",
    "        key=lambda x: x[1]\n",
    "    )[0] for i in range(len(corpus))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert lda transformed matrix to numpy dense representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(536, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.matutils import corpus2dense\n",
    "# use corpus2dense to convert\n",
    "X_lda = corpus2dense(lda[corpus], num_docs=len(corpus), num_terms=5).transpose()\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Queries in gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kevin/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 0.31030023, 0.0178149 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "# create index for cosine similarity measure\n",
    "index_lda = MatrixSimilarity(corpus=lda[corpus])\n",
    "# query a list of similarities between corpus[0] and every doc in the entire corpus (649 docs)\n",
    "sims_list = index_lda[lda[corpus[0]]]\n",
    "sims_list[:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
